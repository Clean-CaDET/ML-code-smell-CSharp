{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f792a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_excel(\"../Dataset/DataSet_Large Class - Round 3.xlsx\")\n",
    "cleanup_nums = {\"Final annotation\":     {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 0: 0}}\n",
    "df= df.replace(cleanup_nums)\n",
    "\n",
    "df_train = pickle.load(open(\"../Dataset/Large_Class_train_set.pickle\", 'rb'))\n",
    "y_train = pickle.load(open(\"../Dataset/Large_Class_train_set_y.pickle\", 'rb'))\n",
    "\n",
    "df_test = pickle.load(open(\"../Dataset/Large_Class_test_set.pickle\", 'rb'))\n",
    "y_test = pickle.load(open(\"../Dataset/Large_Class_test_set_y.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe95bc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       678\n",
      "           1       1.00      0.18      0.30       242\n",
      "\n",
      "    accuracy                           0.78       920\n",
      "   macro avg       0.89      0.59      0.59       920\n",
      "weighted avg       0.83      0.78      0.72       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87       542\n",
      "           1       1.00      0.19      0.32       194\n",
      "\n",
      "    accuracy                           0.79       736\n",
      "   macro avg       0.89      0.60      0.60       736\n",
      "weighted avg       0.83      0.79      0.73       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       136\n",
      "           1       1.00      0.12      0.22        48\n",
      "\n",
      "    accuracy                           0.77       184\n",
      "   macro avg       0.88      0.56      0.54       184\n",
      "weighted avg       0.83      0.77      0.70       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_1(df, y):\n",
    "    # A. Trifu, and R. Marinescu, “Diagnosing design problems in object oriented systems,” In 12th Working Conference on Reverse Engineering (WCRE'05), pp. 10-pp, IEEE, 2005.\n",
    "    # ([ATFD] > 2) & ([WMC] ≥ 47) & ([TCC] < 0.33)\n",
    "    y_pred = (df['ATFD'] > 2) & (df['WMC'] >= 47) & (df['TCC'] < 0.33)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lc_1(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_1(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_1(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68b22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       678\n",
      "           1       1.00      0.14      0.24       242\n",
      "\n",
      "    accuracy                           0.77       920\n",
      "   macro avg       0.88      0.57      0.55       920\n",
      "weighted avg       0.83      0.77      0.70       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       542\n",
      "           1       1.00      0.14      0.24       194\n",
      "\n",
      "    accuracy                           0.77       736\n",
      "   macro avg       0.88      0.57      0.56       736\n",
      "weighted avg       0.83      0.77      0.70       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       136\n",
      "           1       1.00      0.12      0.22        48\n",
      "\n",
      "    accuracy                           0.77       184\n",
      "   macro avg       0.88      0.56      0.54       184\n",
      "weighted avg       0.83      0.77      0.70       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_2(df, y):\n",
    "    # I. Macia, J. Garcia, D. Popescu, A. Garcia, N. Medvidovic, and A. von Staa, “Are automatically-detected code anomalies rele-vant to architectural modularity? An exploratory analysis of evolving systems,” In Proceedings of the 11th annual international conference on Aspect-oriented Software Development, pp. 167-178, 2012.\n",
    "    # ([ATFD] > 5) & ([WMC] ≥ 47) & ([TCC] < 0.33)\n",
    "    y_pred = (df['ATFD'] > 5) & (df['WMC'] >= 47) & (df['TCC'] < 0.3)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lc_2(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_2(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_2(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f376fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       678\n",
      "           1       0.91      0.26      0.41       242\n",
      "\n",
      "    accuracy                           0.80       920\n",
      "   macro avg       0.85      0.63      0.64       920\n",
      "weighted avg       0.82      0.80      0.76       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       542\n",
      "           1       0.93      0.27      0.42       194\n",
      "\n",
      "    accuracy                           0.80       736\n",
      "   macro avg       0.86      0.63      0.65       736\n",
      "weighted avg       0.83      0.80      0.76       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       136\n",
      "           1       0.85      0.23      0.36        48\n",
      "\n",
      "    accuracy                           0.79       184\n",
      "   macro avg       0.81      0.61      0.62       184\n",
      "weighted avg       0.80      0.79      0.74       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_3(df, y):\n",
    "    # Kiefer, A. Bernstein, and J. Tappolet, “Mining software re-positories with isparol and a software evolution ontology,” In Fourth International Workshop on Mining Software Repositories (MSR'07: ICSE Workshops 2007), pp. 10-10, IEEE, 2007.\n",
    "    # [NOM] > 15|[NOF] > 15    \n",
    "    y_pred = (df['NMD'] > 15) | (df['NAD'] > 15)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lc_3(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_3(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_3(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f5d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.87       678\n",
      "           1       0.72      0.38      0.50       242\n",
      "\n",
      "    accuracy                           0.80       920\n",
      "   macro avg       0.77      0.67      0.69       920\n",
      "weighted avg       0.79      0.80      0.78       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       542\n",
      "           1       0.75      0.40      0.52       194\n",
      "\n",
      "    accuracy                           0.81       736\n",
      "   macro avg       0.78      0.68      0.70       736\n",
      "weighted avg       0.80      0.81      0.79       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85       136\n",
      "           1       0.60      0.31      0.41        48\n",
      "\n",
      "    accuracy                           0.77       184\n",
      "   macro avg       0.70      0.62      0.63       184\n",
      "weighted avg       0.74      0.77      0.74       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_4(df, y):\n",
    "    # Danphitsanuphan, and T. Suwantada, “Code smell detecting tool and code smell-structure bug relationship,” In 2012 Spring Congress on Engineering and Technology, pp. 1-5, IEEE, 2012.\n",
    "    # ([NOM] > 20)|([NOF] > 9)|([CLOC] > 750)\n",
    "    y_pred = (df['NMD'] > 20) | (df['NAD'] > 9) | (df['CLOC'] > 750)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "print('All:')\n",
    "lc_4(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_4(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_4(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e5ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       678\n",
      "           1       0.95      0.33      0.49       242\n",
      "\n",
      "    accuracy                           0.82       920\n",
      "   macro avg       0.88      0.66      0.69       920\n",
      "weighted avg       0.84      0.82      0.78       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       542\n",
      "           1       0.96      0.33      0.49       194\n",
      "\n",
      "    accuracy                           0.82       736\n",
      "   macro avg       0.88      0.66      0.69       736\n",
      "weighted avg       0.85      0.82      0.78       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89       136\n",
      "           1       0.94      0.31      0.47        48\n",
      "\n",
      "    accuracy                           0.82       184\n",
      "   macro avg       0.87      0.65      0.68       184\n",
      "weighted avg       0.84      0.82      0.78       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_5(df, y):\n",
    "    # A.M. Fard, and A. Mesbah, “Jsnose: Detecting javascript code smells,” In 2013 IEEE 13th international working conference on Source Code Analysis and Manipulation (SCAM), pp. 116-125, IEEE, 2013.\n",
    "    # [CLOC] > 750|[NOM] + [NOF] > 20  \n",
    "    y_pred = (df['CLOC'] > 750) | (df['NAD'] + df['NMD'] > 20)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "print('All:')\n",
    "lc_5(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_5(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_5(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f900dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       678\n",
      "           1       0.95      0.33      0.49       242\n",
      "\n",
      "    accuracy                           0.82       920\n",
      "   macro avg       0.88      0.66      0.69       920\n",
      "weighted avg       0.84      0.82      0.78       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       542\n",
      "           1       0.96      0.33      0.49       194\n",
      "\n",
      "    accuracy                           0.82       736\n",
      "   macro avg       0.88      0.66      0.69       736\n",
      "weighted avg       0.85      0.82      0.78       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89       136\n",
      "           1       0.94      0.31      0.47        48\n",
      "\n",
      "    accuracy                           0.82       184\n",
      "   macro avg       0.87      0.65      0.68       184\n",
      "weighted avg       0.84      0.82      0.78       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_6(df, y):\n",
    "    # Moha N, Gueheneuc Y-G, Duchien A-F. Decor: a method for the specification and detection of code and design smells.\n",
    "    # IEEE Trans Software Eng (TSE). 2010;36(1):20-36\n",
    "    # [NOM] + [NOF] > 20   \n",
    "    y_pred = df['NAD'] + df['NMD'] > 20\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "print('All:')\n",
    "lc_6(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_6(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_6(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b810cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.74      0.81       678\n",
      "           1       0.50      0.73      0.59       242\n",
      "\n",
      "    accuracy                           0.74       920\n",
      "   macro avg       0.69      0.74      0.70       920\n",
      "weighted avg       0.78      0.74      0.75       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.80       542\n",
      "           1       0.50      0.77      0.61       194\n",
      "\n",
      "    accuracy                           0.74       736\n",
      "   macro avg       0.70      0.75      0.71       736\n",
      "weighted avg       0.79      0.74      0.75       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81       136\n",
      "           1       0.49      0.58      0.53        48\n",
      "\n",
      "    accuracy                           0.73       184\n",
      "   macro avg       0.67      0.69      0.67       184\n",
      "weighted avg       0.75      0.73      0.74       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_7(df, y):\n",
    "    # Lerthathairat P, Prompoon N. An approach for source code classification to enhance maintainability. In\n",
    "    # Proceedings of the Eighth International Joint Conference on Computer Science and Software Engineering. IEEE; 2011:319-324.\n",
    "    # [LCOM3] > 0.8|[LCOM4] > 1.0|[NOF] > 20| [NOM] > 20    \n",
    "    y_pred = (df['LCOM3'] > 0.8) | (df['LCOM4'] > 1) | (df['NAD'] > 20) | (df['NMD'] > 20)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "print('All:')\n",
    "lc_7(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_7(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_7(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc3cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       678\n",
      "           1       1.00      0.13      0.23       242\n",
      "\n",
      "    accuracy                           0.77       920\n",
      "   macro avg       0.88      0.57      0.55       920\n",
      "weighted avg       0.83      0.77      0.70       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       542\n",
      "           1       1.00      0.14      0.24       194\n",
      "\n",
      "    accuracy                           0.77       736\n",
      "   macro avg       0.88      0.57      0.56       736\n",
      "weighted avg       0.83      0.77      0.70       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       136\n",
      "           1       1.00      0.10      0.19        48\n",
      "\n",
      "    accuracy                           0.77       184\n",
      "   macro avg       0.88      0.55      0.53       184\n",
      "weighted avg       0.82      0.77      0.69       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_8(df, y):\n",
    "    # Kaur S, Maini R. Analysis of various software metrics used to detect bad smells.\n",
    "    # Int J Eng Sci (IJES). 2016;5(6):14-20.    \n",
    "    y_pred = (df['LCOM'] >= 0.725) & (df['WMC'] >= 34) & (df['NAD'] >= 8) & (df['NMD'] >= 14)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "print('All:')\n",
    "lc_8(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_8(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_8(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6785dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       678\n",
      "           1       0.73      0.95      0.83       242\n",
      "\n",
      "    accuracy                           0.90       920\n",
      "   macro avg       0.86      0.92      0.88       920\n",
      "weighted avg       0.92      0.90      0.90       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       542\n",
      "           1       0.74      0.95      0.83       194\n",
      "\n",
      "    accuracy                           0.90       736\n",
      "   macro avg       0.86      0.92      0.88       736\n",
      "weighted avg       0.92      0.90      0.90       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92       136\n",
      "           1       0.70      0.98      0.82        48\n",
      "\n",
      "    accuracy                           0.89       184\n",
      "   macro avg       0.85      0.92      0.87       184\n",
      "weighted avg       0.92      0.89      0.89       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_9(df, y):\n",
    "    #  Liu H, Ma Z, Shao W, Niu Z. Schedule of bad smell detection and resolution: a new way to save effort.\n",
    "    #  IEEE Trans Software Eng. 2012;38(1):220-235.\n",
    "    # [CLOC] > 100|[VG] > 20\n",
    "    y_pred = (df['CLOC'] > 100) | (df['WMC'] > 20)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "print('All:')\n",
    "lc_9(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_9(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_9(df_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
