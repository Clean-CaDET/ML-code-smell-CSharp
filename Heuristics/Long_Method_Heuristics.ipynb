{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7a9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_excel(\"../Dataset/DataSet_Long Method - Round 3.xlsx\")\n",
    "cleanup_nums = {\"Final annotation\":     {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 0: 0}}\n",
    "df= df.replace(cleanup_nums)\n",
    "\n",
    "df_train = pickle.load(open(\"../Dataset/Long_Method_train_set.pickle\", 'rb'))\n",
    "y_train = pickle.load(open(\"../Dataset/Long_Method_train_set_y.pickle\", 'rb'))\n",
    "\n",
    "df_test = pickle.load(open(\"../Dataset/Long_Method_test_set.pickle\", 'rb'))\n",
    "y_test = pickle.load(open(\"../Dataset/Long_Method_test_set_y.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91af9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87      1919\n",
      "           1       0.99      0.15      0.27       655\n",
      "\n",
      "    accuracy                           0.78      2574\n",
      "   macro avg       0.88      0.58      0.57      2574\n",
      "weighted avg       0.83      0.78      0.72      2574\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87      1535\n",
      "           1       0.99      0.15      0.26       524\n",
      "\n",
      "    accuracy                           0.78      2059\n",
      "   macro avg       0.88      0.57      0.57      2059\n",
      "weighted avg       0.83      0.78      0.72      2059\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88       384\n",
      "           1       1.00      0.18      0.30       131\n",
      "\n",
      "    accuracy                           0.79       515\n",
      "   macro avg       0.89      0.59      0.59       515\n",
      "weighted avg       0.84      0.79      0.73       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lm_1(df, y):\n",
    "    # P.P. Souza, B.L. Sousa, K.A. Ferreira, and M.A. Bigonha, “Ap-plying software metric thresholds for detection of bad smells,” In Proceedings of the 11th Brazilian Symposium on Software Compo-nents, Architectures, and Reuse, pp. 1-10, 2017.    \n",
    "    # [MLOC] > 30 & [VG] > 4 & [NDB] > 3    \n",
    "    y_pred = (df['MLOC'] > 30) & (df['CYCLO'] > 4) & (df['MMNB'] > 3)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lm_1(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lm_1(df_train, y_train)\n",
    "print('Test:')\n",
    "lm_1(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a9f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90      1919\n",
      "           1       0.87      0.43      0.58       655\n",
      "\n",
      "    accuracy                           0.84      2574\n",
      "   macro avg       0.85      0.71      0.74      2574\n",
      "weighted avg       0.84      0.84      0.82      2574\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90      1535\n",
      "           1       0.87      0.43      0.58       524\n",
      "\n",
      "    accuracy                           0.84      2059\n",
      "   macro avg       0.85      0.71      0.74      2059\n",
      "weighted avg       0.84      0.84      0.82      2059\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90       384\n",
      "           1       0.89      0.43      0.58       131\n",
      "\n",
      "    accuracy                           0.84       515\n",
      "   macro avg       0.86      0.70      0.74       515\n",
      "weighted avg       0.85      0.84      0.82       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lm_2(df, y):\n",
    "    # Fard AM, Mesbah A. Jsnose: detecting javascript code smells. In Proceedings of the IEEE 13th International\n",
    "    # Working Conference on Source Code Analysis and Manipulation. IEEE; 2013:116-125.\n",
    "    y_pred = (df['MLOC'] > 50)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lm_2(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lm_2(df_train, y_train)\n",
    "print('Test:')\n",
    "lm_2(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc84b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      1919\n",
      "           1       0.86      0.50      0.63       655\n",
      "\n",
      "    accuracy                           0.85      2574\n",
      "   macro avg       0.85      0.74      0.77      2574\n",
      "weighted avg       0.85      0.85      0.84      2574\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      1535\n",
      "           1       0.85      0.51      0.63       524\n",
      "\n",
      "    accuracy                           0.85      2059\n",
      "   macro avg       0.85      0.74      0.77      2059\n",
      "weighted avg       0.85      0.85      0.84      2059\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91       384\n",
      "           1       0.88      0.50      0.63       131\n",
      "\n",
      "    accuracy                           0.85       515\n",
      "   macro avg       0.86      0.74      0.77       515\n",
      "weighted avg       0.86      0.85      0.84       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lm_3(df, y):\n",
    "    #H. Liu, Z. Ma, W. Shao, and Z. Niu, “Schedule of bad smell detection and resolution: A new way to save effort,” IEEE trans-actions on Software Engineering, vol. 38 no. 1, pp. 220-235, 2011.\n",
    "    # MLOC > 50 | VG > 10\n",
    "    y_pred = (df['MLOC'] > 50) | (df['CYCLO'] > 10)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lm_3(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lm_3(df_train, y_train)\n",
    "print('Test:')\n",
    "lm_3(df_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
