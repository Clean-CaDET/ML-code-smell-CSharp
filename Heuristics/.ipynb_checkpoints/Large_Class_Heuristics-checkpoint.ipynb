{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77347142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_excel(\"../Dataset/DataSet_Large Class - Round 3.xlsx\")\n",
    "cleanup_nums = {\"Final annotation\":     {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 0: 0}}\n",
    "df= df.replace(cleanup_nums)\n",
    "\n",
    "df_train = pickle.load(open(\"../Dataset/Large_Class_train_set.pickle\", 'rb'))\n",
    "y_train = pickle.load(open(\"../Dataset/Large_Class_train_set_y.pickle\", 'rb'))\n",
    "\n",
    "df_test = pickle.load(open(\"../Dataset/Large_Class_test_set.pickle\", 'rb'))\n",
    "y_test = pickle.load(open(\"../Dataset/Large_Class_test_set_y.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcf12aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       678\n",
      "           1       1.00      0.18      0.30       242\n",
      "\n",
      "    accuracy                           0.78       920\n",
      "   macro avg       0.89      0.59      0.59       920\n",
      "weighted avg       0.83      0.78      0.72       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87       542\n",
      "           1       1.00      0.19      0.32       194\n",
      "\n",
      "    accuracy                           0.79       736\n",
      "   macro avg       0.89      0.60      0.60       736\n",
      "weighted avg       0.83      0.79      0.73       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       136\n",
      "           1       1.00      0.12      0.22        48\n",
      "\n",
      "    accuracy                           0.77       184\n",
      "   macro avg       0.88      0.56      0.54       184\n",
      "weighted avg       0.83      0.77      0.70       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_1(df, y):\n",
    "    # A. Trifu, and R. Marinescu, “Diagnosing design problems in object oriented systems,” In 12th Working Conference on Reverse Engineering (WCRE'05), pp. 10-pp, IEEE, 2005.\n",
    "    # ([ATFD] > 2) & ([WMC] ≥ 47) & ([TCC] < 0.33)\n",
    "    y_pred = (df['ATFD'] > 2) & (df['WMC'] >= 47) & (df['TCC'] < 0.33)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lc_1(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_1(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_1(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1470c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       678\n",
      "           1       1.00      0.18      0.30       242\n",
      "\n",
      "    accuracy                           0.78       920\n",
      "   macro avg       0.89      0.59      0.59       920\n",
      "weighted avg       0.83      0.78      0.72       920\n",
      "\n",
      "Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87       542\n",
      "           1       1.00      0.19      0.32       194\n",
      "\n",
      "    accuracy                           0.79       736\n",
      "   macro avg       0.89      0.60      0.60       736\n",
      "weighted avg       0.83      0.79      0.73       736\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       136\n",
      "           1       1.00      0.12      0.22        48\n",
      "\n",
      "    accuracy                           0.77       184\n",
      "   macro avg       0.88      0.56      0.54       184\n",
      "weighted avg       0.83      0.77      0.70       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lc_2(df, y):\n",
    "    # I. Macia, J. Garcia, D. Popescu, A. Garcia, N. Medvidovic, and A. von Staa, “Are automatically-detected code anomalies rele-vant to architectural modularity? An exploratory analysis of evolving systems,” In Proceedings of the 11th annual international conference on Aspect-oriented Software Development, pp. 167-178, 2012.\n",
    "    # ([ATFD] > 5) & ([WMC] ≥ 47) & ([TCC] < 0.33)\n",
    "    y_pred = (df['ATFD'] > 5) & (df['WMC'] >= 47) & (df['TCC'] < 0.33)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "print('All:')\n",
    "lc_1(df, df['Final annotation'])\n",
    "print('Train:')\n",
    "lc_1(df_train, y_train)\n",
    "print('Test:')\n",
    "lc_1(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_3(nom, nof):\n",
    "    # Kiefer, A. Bernstein, and J. Tappolet, “Mining software re-positories with isparol and a software evolution ontology,” In Fourth International Workshop on Mining Software Repositories (MSR'07: ICSE Workshops 2007), pp. 10-10, IEEE, 2007.\n",
    "    # [NOM] > 15|[NOF] > 15\n",
    "    return (nom > 15) or (nof > 15)\n",
    "    y_pred = (df['NMD'] > 5) & (df['NOF'])\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_4(cloc, nom, nof):\n",
    "    # Danphitsanuphan, and T. Suwantada, “Code smell detecting tool and code smell-structure bug relationship,” In 2012 Spring Congress on Engineering and Technology, pp. 1-5, IEEE, 2012.\n",
    "    # ([NOM] > 20)|([NOF] > 9)|([CLOC] > 750)\n",
    "    return (nom > 20) or (nof > 9) or (cloc > 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_5(cloc, nom, nof):\n",
    "    # A.M. Fard, and A. Mesbah, “Jsnose: Detecting javascript code smells,” In 2013 IEEE 13th international working conference on Source Code Analysis and Manipulation (SCAM), pp. 116-125, IEEE, 2013.\n",
    "    # [CLOC] > 750|[NOM] + [NOF] > 20\n",
    "    return (cloc > 750) or ((nom + nof) > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad24cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_6(nom, nof):\n",
    "    # Moha N, Gueheneuc Y-G, Duchien A-F. Decor: a method for the specification and detection of code and design smells.\n",
    "    # IEEE Trans Software Eng (TSE). 2010;36(1):20-36\n",
    "    # [NOM] + [NOF] > 20\n",
    "    return (nom + nof) > 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_7(lcom3, lcom4, nof, nom):\n",
    "    # Lerthathairat P, Prompoon N. An approach for source code classification to enhance maintainability. In\n",
    "    # Proceedings of the Eighth International Joint Conference on Computer Science and Software Engineering. IEEE; 2011:319-324.\n",
    "    # [LCOM3] > 0.8|[LCOM4] > 1.0|[NOF] > 20| [NOM] > 20\n",
    "    return (lcom3 > 0.8) or (lcom4 > 1) or (nof > 20) or (nom > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8112cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_8(lcom, wmc, nof, nom):\n",
    "    # Kaur S, Maini R. Analysis of various software metrics used to detect bad smells.\n",
    "    # Int J Eng Sci (IJES). 2016;5(6):14-20.\n",
    "    # [LCOM] ≥ 0,725 & [WMC] ≥ 34 & [NOF] ≥ 8 & [NOM] ≥ 14\n",
    "    return (lcom >= 0.725) and (wmc >= 34) or (nof >= 8) or (nom >= 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_9(cloc, vg):\n",
    "    #  Liu H, Ma Z, Shao W, Niu Z. Schedule of bad smell detection and resolution: a new way to save effort.\n",
    "    #  IEEE Trans Software Eng. 2012;38(1):220-235.\n",
    "    # [CLOC] > 100|[VG] > 20\n",
    "    return (cloc > 100) or (vg > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_heuristics(df, y):\n",
    "    for index, row in df.iterrows():    \n",
    "        cloc = row['CLOC']\n",
    "        nom = row['NMD']\n",
    "        nof = row['NAD']\n",
    "        atfd = row['ATFD']\n",
    "        wmc = row['WMC']\n",
    "        tcc = row['TCC']\n",
    "        lcom = row['LCOM']\n",
    "        lcom3 = row['LCOM3']\n",
    "        lcom4 = row['LCOM4']\n",
    "        #print(lc_1(atfd, wmc, tcc))\n",
    "        print(classification_report(y, lc_1(atfd, wmc, tcc)))\n",
    "        #df[LC2] = lc_2(atfd, wmc, tcc)\n",
    "        #df[LC3] = lc_3(nom, nof)\n",
    "        #df[LC4] = lc_4(cloc, nom, nof)\n",
    "        #df[LC5] = lc_5(cloc, nom, nof)\n",
    "        #df[LC6] = lc_6(nom, nof)\n",
    "        #df[LC7] = lc_7(lcom3, lcom4, nof, nom)\n",
    "        #df[LC8] = lc_8(lcom, wmc, nof, nom)\n",
    "        #df[LC9] = lc_9(cloc, vg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b71924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_heuristics(df, df['Final annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5157e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
